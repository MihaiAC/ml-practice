{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6933b021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from typing import List, Set, Dict\n",
    "from scipy.sparse import dok_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58beb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"C:/Users/alexm/Desktop/word_alignments/europarl-v7.sv-en.en\", 'r', encoding='utf8')\n",
    "en_corp = f.readlines()\n",
    "f.close()\n",
    "\n",
    "g = open(\"C:/Users/alexm/Desktop/word_alignments/europarl-v7.sv-en.sv\", 'r', encoding='utf8')\n",
    "sv_corp = g.readlines()\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba3aa5",
   "metadata": {},
   "source": [
    "## Dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cdc2b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1862234\n",
      "1862234\n"
     ]
    }
   ],
   "source": [
    "print(len(en_corp))\n",
    "print(len(sv_corp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53306d73",
   "metadata": {},
   "source": [
    "Since my naive implementation of the word alignment algorithm seems to take a lot of time, I will restrict the corpora to the first 100k lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12c84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corp = en_corp[:100000]\n",
    "sv_corp = sv_corp[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc0fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique tokens in the two corpora.\n",
    "def get_unique_tokens(corp: List[str]) -> Set[str]:\n",
    "    unique_tokens = set()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\'?\\w*')\n",
    "    for line in corp:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        for token in tokens:\n",
    "            if token != '':\n",
    "                unique_tokens.add(token)\n",
    "    return unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3be926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_unique_tokens = get_unique_tokens(en_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5417129",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_unique_tokens = get_unique_tokens(sv_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23acf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming tokens to int IDs so we can use a sparse matrix in the algorithm.\n",
    "def assign_token_ids(unique_tokens: Set[str]) -> Dict[str, int]:\n",
    "    curr_id = 0\n",
    "    token_ids = dict()\n",
    "    for token in unique_tokens:\n",
    "        token_ids[token] = curr_id\n",
    "        curr_id += 1\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c092e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_token_ids = assign_token_ids(en_unique_tokens)\n",
    "sv_token_ids = assign_token_ids(sv_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0181b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the English corpus: \n",
      "32330\n",
      "Number of unique tokens in the Swedish corpus: \n",
      "71260\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique tokens in the English corpus: \\n\" + str(len(en_unique_tokens)))\n",
    "print(\"Number of unique tokens in the Swedish corpus: \\n\" + str(len(sv_unique_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83b37621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2303835800\n"
     ]
    }
   ],
   "source": [
    "print(len(en_unique_tokens) * len(sv_unique_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2cbf8",
   "metadata": {},
   "source": [
    "If we hadn't truncated the corpora, we would have had 300B maximum possible word alignments. This way, the number is reduced to 2B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b1ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_corpus(corp: List[str]) -> List[List[str]]:\n",
    "    tokenised_corp = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\'?\\w*')\n",
    "    for line in corp:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        tokenised_corp += [tokens]\n",
    "    return tokenised_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "597486c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenised_corp = tokenise_corpus(en_corp)\n",
    "sv_tokenised_corp = tokenise_corpus(sv_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab4e537d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Madam',\n",
       " 'President',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'wish',\n",
       " 'to',\n",
       " 'reopen',\n",
       " 'the',\n",
       " 'debate',\n",
       " 'but',\n",
       " 'I',\n",
       " 'had',\n",
       " 'also',\n",
       " 'asked',\n",
       " 'for',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'to',\n",
       " 'comment',\n",
       " 'on',\n",
       " 'Mr',\n",
       " 'Bar√≥n',\n",
       " \"Crespo's\",\n",
       " 'motion']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenised_corp[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c7406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4033118158854897e-05\n"
     ]
    }
   ],
   "source": [
    "theta_init = 1/len(sv_unique_tokens)\n",
    "print(theta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09202d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures needed for the algorithm.\n",
    "counts = dict()\n",
    "counts_english = dict()\n",
    "params = dok_matrix((len(sv_unique_tokens),len(en_unique_tokens)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0085338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 started.\n",
      "0\n",
      "0.00049591064453125\n",
      "10000\n",
      "7.0302910804748535\n",
      "20000\n",
      "14.689014434814453\n",
      "30000\n",
      "22.645759105682373\n",
      "40000\n",
      "30.82577681541443\n",
      "50000\n",
      "39.21361780166626\n",
      "60000\n",
      "47.932785511016846\n",
      "70000\n",
      "57.48820948600769\n",
      "80000\n",
      "67.04710459709167\n",
      "90000\n",
      "76.79299092292786\n",
      "Iteration 1 started.\n",
      "0\n",
      "246.7475836277008\n",
      "10000\n",
      "436.6368787288666\n",
      "20000\n",
      "632.462290763855\n",
      "30000\n",
      "831.3624000549316\n",
      "40000\n",
      "1010.1958744525909\n",
      "50000\n",
      "1178.241361618042\n",
      "60000\n",
      "1335.6378767490387\n",
      "70000\n",
      "1508.6536049842834\n",
      "80000\n",
      "1677.3288881778717\n",
      "90000\n",
      "1841.9046199321747\n",
      "Iteration 2 started.\n",
      "0\n",
      "2151.2088871002197\n",
      "10000\n",
      "2312.7137899398804\n",
      "20000\n",
      "2484.801346540451\n",
      "30000\n",
      "2654.1228511333466\n",
      "40000\n",
      "2806.119827270508\n",
      "50000\n",
      "2958.5612194538116\n",
      "60000\n",
      "3128.178550720215\n",
      "70000\n",
      "3324.0444185733795\n",
      "80000\n",
      "3521.647987127304\n",
      "90000\n",
      "3716.576631784439\n",
      "Iteration 3 started.\n",
      "0\n",
      "4086.761109352112\n",
      "10000\n",
      "4280.202750921249\n",
      "20000\n",
      "4475.504882335663\n",
      "30000\n",
      "4672.576740503311\n",
      "40000\n",
      "4849.30916261673\n",
      "50000\n",
      "5029.074121952057\n",
      "60000\n",
      "5215.2483830451965\n",
      "70000\n",
      "5415.835385560989\n",
      "80000\n",
      "5614.933400154114\n",
      "90000\n",
      "5808.809535980225\n",
      "Iteration 4 started.\n",
      "0\n",
      "6175.901954174042\n",
      "10000\n",
      "6367.74449300766\n",
      "20000\n",
      "6566.243344545364\n",
      "30000\n",
      "6768.878819942474\n",
      "40000\n",
      "6947.864565849304\n",
      "50000\n",
      "7124.941210269928\n",
      "60000\n",
      "7309.619041442871\n",
      "70000\n",
      "7508.682335615158\n",
      "80000\n",
      "7706.877135276794\n",
      "90000\n",
      "7899.485000371933\n",
      "Total elapsed time: 8262.325767\n"
     ]
    }
   ],
   "source": [
    "# First step of the IBM Model 1 alignment algorithm.\n",
    "start_time = time.time()\n",
    "\n",
    "nr_iterations = 5\n",
    "for iteration in range(nr_iterations):\n",
    "    print(\"Iteration %i started.\"%(iteration))\n",
    "    \n",
    "    # Reset the counts.\n",
    "    counts = dict()\n",
    "    counts_english = dict()\n",
    "    \n",
    "    for ii in range(len(en_tokenised_corp)):\n",
    "        if ii % 10000 == 0:\n",
    "            print(ii)\n",
    "            print(time.time() - start_time)\n",
    "\n",
    "        for sv_word in sv_tokenised_corp[ii]:\n",
    "            normalisation_term = 0\n",
    "            \n",
    "            for en_word in en_tokenised_corp[ii]:\n",
    "                if iteration == 0:\n",
    "                    normalisation_term += theta_init\n",
    "                else:\n",
    "                    normalisation_term += params[sv_token_ids[sv_word], en_token_ids[en_word]]\n",
    "\n",
    "            for en_word in en_tokenised_corp[ii]:\n",
    "                if iteration == 0:\n",
    "                    expected_count = theta_init/normalisation_term\n",
    "                else:\n",
    "                    expected_count = params[sv_token_ids[sv_word], en_token_ids[en_word]]/normalisation_term\n",
    "\n",
    "                if (sv_word, en_word) in counts:\n",
    "                    counts[(sv_word, en_word)] += expected_count\n",
    "                else:\n",
    "                    counts[(sv_word, en_word)] = expected_count\n",
    "\n",
    "                if en_word in counts_english:\n",
    "                    counts_english[en_word] += expected_count\n",
    "                else:\n",
    "                    counts_english[en_word] = expected_count\n",
    "\n",
    "    for (sv_word, en_word) in counts:\n",
    "        params[sv_token_ids[sv_word], en_token_ids[en_word]] = counts[(sv_word, en_word)]/counts_english[en_word]\n",
    "\n",
    "print(\"Total elapsed time: %f\"%(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491de02",
   "metadata": {},
   "source": [
    "Most probable alignment for the words in the first 10 sentences after 1 iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e707747d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption - √Öterupptagande\n",
      "of - av\n",
      "the - av\n",
      "session - sessionen\n",
      "I - jag\n",
      "declare - f√∂rklarar\n",
      "resumed - efter\n",
      "the - att\n",
      "session - session\n",
      "of - att\n",
      "the - att\n",
      "European - att\n",
      "Parliament - Europaparlamentets\n",
      "adjourned - Europaparlamentets\n",
      "on - p√•\n",
      "Friday - p√•\n",
      "17 - 17\n",
      "December - december\n",
      "1999 - och\n",
      "and - och\n",
      "I - jag\n",
      "would - vill\n",
      "like - vill\n",
      "once - en\n",
      "again - nytt\n",
      "to - att\n",
      "wish - vill\n",
      "you - ni\n",
      "a - en\n",
      "happy - att\n",
      "new - nytt\n",
      "year - √•r\n",
      "in - att\n",
      "the - att\n",
      "hope - hoppas\n",
      "that - att\n",
      "you - ni\n",
      "enjoyed - haft\n",
      "a - en\n",
      "pleasant - jag\n",
      "festive - gott\n",
      "period - den\n",
      "Although - den\n",
      "as - som\n",
      "you - ni\n",
      "will - i\n",
      "have - har\n",
      "seen - har\n",
      "the - i\n",
      "dreaded - kunnat\n",
      "millennium - 2000\n",
      "bug' - buggen\n",
      "failed - har\n",
      "to - som\n",
      "materialise - stora\n",
      "still - i\n",
      "the - i\n",
      "people - som\n",
      "in - i\n",
      "a - ett\n",
      "number - antal\n",
      "of - av\n",
      "countries - i\n",
      "suffered - drabbats\n",
      "a - ett\n",
      "series - antal\n",
      "of - av\n",
      "natural - naturkatastrofer\n",
      "disasters - naturkatastrofer\n",
      "that - som\n",
      "truly - verkligen\n",
      "were - som\n",
      "dreadful - f√∂rskr√§ckliga\n",
      "You - Ni\n",
      "have - har\n",
      "requested - beg√§rt\n",
      "a - en\n",
      "debate - debatt\n",
      "on - i\n",
      "this - i\n",
      "subject - √§mnet\n",
      "in - i\n",
      "the - i\n",
      "course - i\n",
      "of - i\n",
      "the - i\n",
      "next - kommande\n",
      "few - under\n",
      "days - dagar\n",
      "during - under\n",
      "this - i\n",
      "part - en\n",
      "session - under\n",
      "In - i\n",
      "the - att\n",
      "meantime - Till\n",
      "I - jag\n",
      "should - att\n",
      "like - vill\n",
      "to - att\n",
      "observe - h√•ller\n",
      "a - en\n",
      "minute' - minut\n",
      "s - i\n",
      "silence - tyst\n",
      "as - som\n",
      "a - en\n",
      "number - antal\n",
      "of - i\n",
      "Members - kolleger\n",
      "have - att\n",
      "requested - beg√§rt\n",
      "on - f√∂r\n",
      "behalf - f√∂r\n",
      "of - i\n",
      "all - att\n",
      "the - att\n",
      "victims - offren\n",
      "concerned - f√∂r\n",
      "particularly - bl\n",
      "those - de\n",
      "of - i\n",
      "the - att\n",
      "terrible - de\n",
      "storms - stormarna\n",
      "in - i\n",
      "the - att\n",
      "various - de\n",
      "countries - l√§nder\n",
      "of - i\n",
      "the - att\n",
      "European - Europeiska\n",
      "Union - unionen\n",
      "Please - ber\n",
      "rise - en\n",
      "then - f√∂r\n",
      "for - f√∂r\n",
      "this - f√∂r\n",
      "minute' - minut\n",
      "s - f√∂r\n",
      "silence - tyst\n",
      "The - en\n",
      "House - en\n",
      "rose - h√∂ll\n",
      "and - en\n",
      "observed - h√∂ll\n",
      "a - en\n",
      "minute' - minut\n",
      "s - en\n",
      "silence - tyst\n",
      "Madam - Fru\n",
      "President - talman\n",
      "on - en\n",
      "a - en\n",
      "point - en\n",
      "of - en\n",
      "order - en\n",
      "You - Ni\n",
      "will - att\n",
      "be - att\n",
      "aware - k√§nner\n",
      "from - fr√•n\n",
      "the - att\n",
      "press - i\n",
      "and - och\n",
      "television - att\n",
      "that - att\n",
      "there - det\n",
      "have - att\n",
      "been - i\n",
      "a - en\n",
      "number - rad\n",
      "of - i\n",
      "bomb - k√§nner\n",
      "explosions - bombexplosioner\n",
      "and - och\n",
      "killings - mord\n",
      "in - i\n",
      "Sri - Sri\n",
      "Lanka - Lanka\n",
      "One - En\n",
      "of - av\n",
      "the - i\n",
      "people - personer\n",
      "assassinated - de\n",
      "very - mycket\n",
      "recently - nyligen\n",
      "in - i\n",
      "Sri - Sri\n",
      "Lanka - Lanka\n",
      "was - var\n",
      "Mr - f√∂r\n",
      "Kumar - Kumar\n",
      "Ponnambalam - Kumar\n",
      "who - som\n",
      "had - var\n",
      "visited - bes√∂kte\n",
      "the - i\n",
      "European - i\n",
      "Parliament - Europaparlamentet\n",
      "just - bara\n",
      "a - som\n",
      "few - n√•gra\n",
      "months - m√•nader\n",
      "ago - sedan\n"
     ]
    }
   ],
   "source": [
    "for ii in range(10):\n",
    "    for en_word in en_tokenised_corp[ii]:\n",
    "        best_prob = -1\n",
    "        best_sv_word = ''\n",
    "        for sv_word in sv_tokenised_corp[ii]:\n",
    "            if params[sv_token_ids[sv_word], en_token_ids[en_word]] > best_prob:\n",
    "                best_prob = params[sv_token_ids[sv_word], en_token_ids[en_word]]\n",
    "                best_sv_word = sv_word\n",
    "        print(en_word + ' - ' + best_sv_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "610640c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Resumption', 'of', 'the', 'session']\n"
     ]
    }
   ],
   "source": [
    "print(en_tokenised_corp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ada1267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['√Öterupptagande', 'av', 'sessionen']\n"
     ]
    }
   ],
   "source": [
    "print(sv_tokenised_corp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fef94420",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption - √Öterupptagande\n",
      "0.4459885\n",
      "Resumption - av\n",
      "0.22032505\n",
      "Resumption - sessionen\n",
      "0.3334858\n",
      "of - √Öterupptagande\n",
      "4.469413e-17\n",
      "of - av\n",
      "0.19555347\n",
      "of - sessionen\n",
      "6.3065104e-12\n",
      "the - √Öterupptagande\n",
      "1.566281e-18\n",
      "the - av\n",
      "0.069594085\n",
      "the - sessionen\n",
      "6.8032455e-12\n",
      "session - √Öterupptagande\n",
      "8.53622e-06\n",
      "session - av\n",
      "0.022391718\n",
      "session - sessionen\n",
      "0.088088855\n"
     ]
    }
   ],
   "source": [
    "for en_word in en_tokenised_corp[0]:\n",
    "    for sv_word in sv_tokenised_corp[0]:\n",
    "        print(en_word + \" - \" + sv_word)\n",
    "        print(params[sv_token_ids[sv_word], en_token_ids[en_word]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d7987",
   "metadata": {},
   "source": [
    "Pretty cool!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
